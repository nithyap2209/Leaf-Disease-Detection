{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithyap2209/Leaf-Disease-Detection/blob/main/leaf_disease_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l-xMlJrfrfl",
        "outputId": "7b516948-0fda-4071-cf5e-94d5f06277f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading leaf-disease-detection-dataset, 1437121598 bytes compressed\n",
            "[==================================================] 1437121598 bytes downloaded\n",
            "Downloaded and uncompressed: leaf-disease-detection-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'leaf-disease-detection-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2272471%2F3814063%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240801%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240801T103130Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D032c095998076e11733a2eb52d047fd74b84c6cbfc6efc3b2b3c08b76f3800aa67ab10001ce4325586b36c65820de1115ec427f21c6cfcf91a02248e92938509b0f2d370bc898c8cbba9885b7589d6990b930e539c0bdda63289693ed965e393ad8002ee57dc088971dc343b2f0a4c200a95cfb9c18a3d1993b730d28e6b64a1c2140558eed91154c0f9b429fb4782f387ba7076b0921b1a33239d2835fe05aef2c58d3e90207b1ec7a5835d7abfbcf5c6086e3d466f43b390ad8130c767483038173ecf62a8617361035ad762593997daa05ae74dd2e98542c5e8be4236b0fab429f6af662f06510bc6096d68bc480873776c6d734d4d493c9fe52d145436b6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GUBwwmkBc3fj"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7k1UBk7bc3sW"
      },
      "outputs": [],
      "source": [
        "# Define data directories\n",
        "train_dir = '/kaggle/input/leaf-disease-detection-dataset/dataset/train'\n",
        "test_dir = '/kaggle/input/leaf-disease-detection-dataset/dataset/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2YOjejWcc3zY"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "input_shape = (128, 128, 3)  # Adjust image dimensions as needed\n",
        "num_classes = len(os.listdir(train_dir))\n",
        "batch_size = 32\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZR9Dxplc32J",
        "outputId": "8d668b84-927a-465f-9c26-96e31a217719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Create a CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "spNwSJQTc35p"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zltLy3dvdTR-",
        "outputId": "86f4d5dc-c4d9-455c-83b5-05ac181e28f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70295 images belonging to 38 classes.\n",
            "Found 17572 images belonging to 38 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ5kAqXBdTUr",
        "outputId": "bbade050-5bc0-463b-9a87-9e33928bbe81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2572s\u001b[0m 1s/step - accuracy: 0.3027 - loss: 2.4966 - val_accuracy: 0.7241 - val_loss: 0.9361\n",
            "Epoch 2/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 19ms/step - accuracy: 0.6875 - loss: 0.8994 - val_accuracy: 1.0000 - val_loss: 0.1546\n",
            "Epoch 3/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2478s\u001b[0m 1s/step - accuracy: 0.6516 - loss: 1.1299 - val_accuracy: 0.7939 - val_loss: 0.6410\n",
            "Epoch 4/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37us/step - accuracy: 0.7188 - loss: 0.7165 - val_accuracy: 1.0000 - val_loss: 0.0409\n",
            "Epoch 5/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2435s\u001b[0m 1s/step - accuracy: 0.7344 - loss: 0.8435 - val_accuracy: 0.8778 - val_loss: 0.3775\n",
            "Epoch 6/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41us/step - accuracy: 0.7812 - loss: 0.7122 - val_accuracy: 1.0000 - val_loss: 0.0545\n",
            "Epoch 7/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2486s\u001b[0m 1s/step - accuracy: 0.7736 - loss: 0.7202 - val_accuracy: 0.8882 - val_loss: 0.3475\n",
            "Epoch 8/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33us/step - accuracy: 0.9062 - loss: 0.4026 - val_accuracy: 0.7500 - val_loss: 2.0875\n",
            "Epoch 9/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2486s\u001b[0m 1s/step - accuracy: 0.8045 - loss: 0.6073 - val_accuracy: 0.9073 - val_loss: 0.2838\n",
            "Epoch 10/10\n",
            "\u001b[1m2196/2196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - accuracy: 0.7812 - loss: 0.7158 - val_accuracy: 1.0000 - val_loss: 0.1269\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44e3IEewdTXp",
        "outputId": "210d8222-19d6-49ca-ab48-f35a8cf189fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 295ms/step - accuracy: 0.9123 - loss: 0.2739\n",
            "Test accuracy: 91.08%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save class names to a file\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "with open('class_names.json', 'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "# Save the model\n",
        "model.save('leaf_disease_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SulFNReJ3b0m",
        "outputId": "3da4e88e-ba25-483d-fd5a-6c2165b74d6d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2Q5F4x0QFzj8"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtxICu5rJ1Vz",
        "outputId": "f2312264-3adb-4a08-a9a4-10d0142abe24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.86.113.56\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sJg0-7biS5BJ"
      },
      "outputs": [],
      "source": [
        "# !pip install streamlit pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnYDSlP8TFTi",
        "outputId": "0bdbcdf1-2bc3-4f08-dd16-7a3188452f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting leaf.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile leaf.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('leaf_disease_model.h5')\n",
        "\n",
        "# Load class names from the JSON file\n",
        "with open('class_names.json', 'r') as f:\n",
        "    class_names = json.load(f)\n",
        "\n",
        "# Define image preprocessing function\n",
        "def preprocess_image(image):\n",
        "    image = image.resize((128, 128))  # Resize image to match model input size\n",
        "    image_array = np.array(image) / 255.0  # Normalize image\n",
        "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
        "    return image_array\n",
        "\n",
        "# Define Streamlit app\n",
        "st.title(\"Leaf Disease Detection\")\n",
        "st.write(\"Upload an image of a leaf to classify its disease.\")\n",
        "\n",
        "# Image upload widget\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Display the image\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Preprocess the image\n",
        "    image_array = preprocess_image(image)\n",
        "\n",
        "    # Predict the class of the image\n",
        "    predictions = model.predict(image_array)\n",
        "    class_index = np.argmax(predictions[0])\n",
        "\n",
        "    # Check if class_index is within the range of available class names\n",
        "    if class_index < len(class_names):\n",
        "        class_name = class_names[class_index]\n",
        "        # Display the result\n",
        "        st.write(f\"Prediction: {class_name}\")\n",
        "        st.write(f\"Confidence: {predictions[0][class_index] * 100:.2f}%\")\n",
        "    else:\n",
        "        st.write(\"Error: The model's class index is out of range.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "jwXXmyTigOh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch2IYW1BWD7w"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'your_authtoken' with your actual Ngrok authtoken\n",
        "ngrok.set_auth_token(\"2k4N3yToOiU2PRzuUnSoCoQfwXs_88KGceXEeQ864w7ABLECz\")\n",
        "\n",
        "# Set up a tunnel to the Streamlit app\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"Streamlit app is live at {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run leaf.py --server.port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQwHzOVuzuH",
        "outputId": "5c21b921-9de3-4b43-9156-4d0d428c7efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at NgrokTunnel: \"https://bf7f-34-86-113-56.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.113.56:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-08-02 10:48:04.629918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-02 10:48:04.968033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-02 10:48:05.066063: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-02 10:48:08.624710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}